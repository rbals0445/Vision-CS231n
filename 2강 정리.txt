Image classification 이란?
이미지를 픽셀들의 배열로 표현하고 거기에 label을 할당하는 작업.

이미지 분류 파이프라인

Input -> learning -> Evaluation 

Input : N개의 이미지 set이 있고 label된 K개의 각각 다른 클래스(카테고리 존재)
이 데이터를 training set으로 활용한다

Learning : 모든 클래스들이 어떻게 생겼는지 training set을 사용해서 학습시킨다.
learning a model or training a classifier 라고 부른다.

Evaluation : label이 뭔지 predict 시켜봄으로써  classifier의 성능을 테스트한다.
참고로 테스트할때는 새로운 set의 이미지를 사용한다(처음보는걸로)
결과가 나오면 예측한거랑 실제이미지를 비교해본다.
ground thruth(실측 자료) 라는 표현을 쓰는데 이것은 classifier랑 비교할 대조본(원본) 을 말한다.

Nearest Neighbor Classifier (최근접 이웃 분류기)
CNN과는 관련이 없고 매우 드물게 사용된다.

그럼 왜 쓰는가?
이미지 분류 문제에 대해서 가장 기본적 접근을 하는 idea를 제공하기 때문이다.

dataset은 CIFAR-10 이라는걸 사용한다. (최종 결과값이 10자리로 나와서 -10인것 같기도)
32*32 pixel의 10개 클래스들이 존재하고 수많은 이미지들이 있다.

training set 5만장, test set 1만장이 있다.

L1 distance ( Manhattan distance)
array화 된 원소들의 픽셀값의 차이의 절댓값 을 모두 더한다.
두개가 같거나 작을수록 0이나 0에 가까울것이고 차이가 클수록 합도 커질것이다.

L2 distance ( euclidean distance)
두 벡터의 차를 제곱한 후 다 더해서 루트를 씌움.


L1 vs L2
L1 : 두 벡터간의 차이가 큰것들
L2 : 두 벡터간의 차이가 미미한것들

L2가 좀 더 벡터의 차이에 크게 반응한다.(unforgiving)
즉 두 벡터간의 차이가 크면 L2보다는 L1을 쓰는것이 좀 더 좋다는 뜻이다.
L2는 여러가지 벡터의 차이가 미미한 경우에 사용하는것이 좋다.

또한 L1,L2 norm은 p-norm 중에 가장 많이 쓰이는 norm이다. https://planetmath.org/vectorpnorm 
여길 참고해도 좋고, 선형대수 저번에 정리한것에 그대로 있다. 좀 더 알아보기 쉬움.

k-Nearest Neighbor Classifier (kNN 분류기)
아이디어는 매우 간단하다 .
training set의 가장 근접한(가까운) 단일 이미지를 찾는것 대신에 top k개의 근접한 이미지를 찾는다. 
그다음 test 이미지에 vote를 한다. k=1 일땐 NN과 똑같다고 보면 된다. (최상위 1개만 쓸거니까)

최상위 이미지가 우연찮게 픽셀값이 비슷해서 이상한게 잡혔다고 쳐보자.
NN을 사용하면 이상한걸 prediction 할것이다.
하지만 k값을 높여줄수록 좀 더 outlier을 제거하는데 좀 더 효율적일것이다. (즉 맞을 확률이 조금이라도 더 높아질 수 있다.)

*outlier : 비정상적으로 벗어난것들. IQ 평균이 50이라고 가정하면 어떤사람만 300일때 그런사람을 걸러야 평균이 좀더 정확해짐.

여기에 색깔이 입혀진 지역은 L2 distance를 사용한 분류기로 만든 decision boundaries(결정 경계) 이다.
하얀색 지역은 모호하게 분류된 경우이다. (예를들면 적어도 2개 이상의 클래스들이 투표수가 같을경우)
NN에는 흰색 경계가 나올 수 없는 이유이기도 하다.

NN을 보면 outlier를 처리하지 못해서 초록색 안에 파란색 섬(island)이 있기도 하고.. 이런것들이 incorrect prediction을 하게 만든다.
반면에 k=5 인 KNN은 이러한 irregularities(outlier들이라고 보면 될 것 같다.) 들이 생기지 않게 좀 더 부드럽게한다.
여기 사진엔 나오지 않지만 test data를 좀 더 일반화 할 수 있다.

5-NN 분류기를 보면 회색부분은 위에서 설명한것 처럼 2개 이상의 색이 같은 표를 받은것이다.
예를들면 2표 red, 2표 blue, 1표 green 이런식.

실제로 kNN을 쓰는게 거의 무조건 좋은것 같다는걸 눈으로 확인했다.
그러나 k를 몇으로 하는게 좋은가? 무조건 k가 높으면 좋은게 아닐까? K를 어떻게 설정할지에 대해서 배워본다.

Hyperparameter 튜닝을 위한 Validation Sets(검증 셋)

kNN은 최적의 k를 요구한다. 하지만 L1 norm, L2 norm 말고도 여러가지 distance function이 존재한다. 또한 내적 등등 여러가지 방법이 있다.

이러한 선택들을 Hyperparameter라고 부른다.그리고 이것들은 data로부터 배워진(학습된) 많은 머신러닝 알고리즘에서 나타난다.
어떤 값을 선택해서 세팅할지는 항상 명확하지 않다.

가장 쉬운 방법은 그냥 여러값들을 넣어서 시도해보고 그 결과를 토대로 선택하는 것이다.
좋은 아이디어이고 실제로 그렇게 할 것이지만 매우 조심히 해야한다고 한다. 왜??

중요! 특히 hyperparameter를 수정할 목적으로 테스트셋을 사용해선 안된다.
(원칙상 or 이론상 ideally)ML 알고리즘을 만들때 test set을 알고리즘 테스트때 딱 한번 외에는 손대서는 안될 정말 소중한 자원으로 생각해야한다. 그렇지 않으면 테스트 셋에 맞추기 위해서 hyperparameter를 수정하는 위험이 있다. 그렇게 하면 실제 모델을 배치했을때 상당한 성능 감소가 나올 수 있다.
-> 테스트셋에서만 굴러가는게 아니고 범용적인걸 고려해야 함.

테스트셋에다가 hyperparameter를 맞추는걸 test set에 overfit 했다고 한다.
다른 관점으로 보면 오히려 test set을 training set처럼 효과적으로 사용했다. 따라서 실제로 모델을 배치했을때 나올 값보다는 너무 optimistic 한 결과가 나올것이다.
-> 좀만 생각해보면 당연하다.. 왜냐? 기출문제를 미리 풀어서 시험을 보면 당연 점수가 잘나온다. 하지만 그 기출문제만 풀 줄 안다면?
다음 시험부터는 어떻게 될것인가.. 당연히 점수가 이전보다 낮게 나올 수 밖에 없다.(일반적으로)

그러나 평가하는 마지막에만 test set을 사용하면 우리가 만든 분류기의 일반화된 성능을 잘 평가하는 척도(proxy)가 될 것이다.

운좋게도 test set을 건들지 않고 올바르게 hyperparameter를 튜닝하는 법이 있다.
그 방법은 우리의 training set을 2개로 나누는 것이다.
1. 미세하게 더 작은 트레이닝 셋 우리는 이것을 (validation set) 이라고 부른다. 한국말로 검증 셋

ex) 5만개의 CIFAR-10 을 예시로 들어보면
49000장은 training set으로 쓰고 1000장은 validation set으로 사용한다.

validation set은 hyper-parameters를 튜닝하기 위한 가짜 test set으로 사용한다.(필수적)

Cross-validation (교차 검증)

training data의 사이즈가 작을경우에는 '교차검증' 이라고 불리는 좀 더 정교한 튜닝 기술을 사용한다
여러가지 validation set을 테스트 한 후 평균적인 성능을 내서 어떤 k가 더 좋고 noise가 적은 결과를 낼지 예측하는것이다.


예를들어 5-fold cross-validation 을 예시로 들면 (5개의 그룹으로 이루어진 교차검증 으로 생각하면 될 것 같다)
training data를 같은 크기의 5개의 folds로 나누고 4개는 training, 1개는 validation 으로 사용한다.
어떤 validation fold를 사용할지에 따라 iterate 하고 성능을 평가하고 다른 fold에 대해서 성능의 평균을 낸다.

<!-- 이해가 잘 안감-->
이 사진은
각각의 k에 대해서 4개의 folds로 트레이닝을 시키고 5번째 fold로 성능을 평가한다.
('A',B,C,D,E), (A,'B',C,D,E) , (A,B,'C',D,E) .. 총 5개 따라서 5개의 accuracy 가 나옴. (n^2 반복)
각 k마다 검증셋으로 활용한 그룹들에서 5개의 정확도가 나온다.
정확도는 y축, 결과는 점이다.
error bar는 표준편차를 나타낸다.
k = 7 일때 제일 높은 accuracy를 가진다.

fold를 올릴수록 좀더 부드러운 곡선(noise가 적은)을 볼 수 있을것이다.
<!-- -->
k-fold cross validation(K 분할 교차 검증)
실제로 사람들은 교차검증을 피하고 하나의 validation set만 가지는걸 선호한다.
왜냐하면 교차검증은 cost가 너무 비싸다(계산할게 많음. 데이터 1억개면 1억개의 제곱이라고 생각하면 됨.)

사람들은 50~90% 정도를 training으로 쓰고 나머지를 validation으로 쓰는 경향이 있다.
그러나 이 %는 여러가지 요인에 의존적이다.(제한이 있다)
만약 hyperparameter가 너무 크면 좀더 큰 validation split을 선호할 것이다. (training set을 줄인다는 말인것 같음)
만약 validation set이 너무 적으면(수백개) 교차검증을 사용하는것이 더 안전하다.
실제로 볼 수 있는 일반적인 개수의 folds는 3개,5개,10개 정도 이다.

Nearest Neighbor classifier의 장,단점

장점 : 구현하고 이해하기 쉽다. training에 시간이 들지 않는다.O(1) (그냥 저장하고 인덱스 매김)
단점 : test time에 드는 연산 cost가 있다. test image하나를 training data 전체랑 비교해야하기 때문에.
실 사용에서는 training time보다 test time의 효율성에 좀 더 주의한다. ( test time이 더 적게 들어야한다. training time은 그렇게 신경 x)
deep neural network는 NN과는 완전 반대이다. training time이 굉장히 길고 test time이 굉장히 짧다.
이런 방식이 실제에서는 좀 더 바람직한 방법이다.

NN classifier의 연산량은 아직도 연구가 되는 주제이다.
ANN (Approximate Nearest Neighbor) 근사 최근접 이웃 알고리즘 이나 라이브러리들이 있어서 이것이 dataset 내에서 NN을 찾는것을 가속화해준다. 
이러한 알고리즘들은 정확도를 조금 잃는대신에 시/공간 복잡도를 크게 이득 볼 수 있다.(NN 검색시에)
kdtree 를 만들거나 k-means 알고리즘을 실행할때 사용하는 전처리 stage에 의존하는 편이다.
* 나는 kdtree와 k-means 알고리즘이 뭔지 모른다.
(다음번에 또 볼때엔 이 부분도 찾아봐야겠다)

최근접 이웃 분류기는 '차원이 낮은 data' 에서 좋은 선택이 될 수 있다.
하지만 거의 실생활 이미지 분류에서는 쓰일 일이 없다.

실제로는 고차원 물체라서 픽셀들도 많고, 고차원에서의 distance는 매우 직관적이지 않다.
아래 사진을 픽셀 기반 L2유사도로 보면 눈으로 보는것곽 매우 많이 다르다.

아래 사진은 원본을 1로 잡고 2,3,4번 사진들이 L2 distance 기준으로 다 같은 거리만큼 떨어져 있다.
(즉 sum이 모두 같다는 말이다)
근데 어떻게 2,3,4가 같은 사진인가.. 눈으로 보기에 다른게 보이는데..
이 예시를 통해서 pixel로 구한 distance는 지각적(perceptual)으로나 의미적으로나 맞지 않다.

픽셀 기반 차이는 부적절한걸 보여주는 다른 예시이다. 아래 사진은 t-SNE 시각화 기법을 사용해서 보여주는 예시이다.
* t-SNE는
https://lvdmaaten.github.io/tsne/
여길 참고하면 되는데 
고차원 dataset의 차원을 축소해주는 기술이라고 나와있다.
위에 CIFAR-10 예시를 2차원 으로 임베딩 해서 보여주는것이다.
근처에 있을수록 L2거리가 굉장히 가깝다.(유사하다)
조금 확대해서 보면 배경이 비슷한 비행기와 트럭이 같이 묶이는등.. 실제 내용의 차이보다는 배경에 많은 영향을 받는걸 볼 수 있다.

가까이 있는 이미지들은 보편적 색깔의 분포나 배경색에 영향을 많이 받는다. 그들의 실제 의미와는 관계없이 (개나 고양이 같은 종류)
그러면 이런 배경이나, 다른 종 에 관계없이 알맞게 clustering 을 이루게 하려면 pixel 을 넘어선 다른 것들이 필요하다.
